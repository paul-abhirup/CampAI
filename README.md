# CampAI [ RAG‑Powered Campaign Analytics Backend  ]
**A production‑grade FastAPI microservice that lets users upload campaign documents (PDF, CSV, JSON, etc.), stores them as vector embeddings in PostgreSQL + pgvector, and answers natural‑language questions with context‑aware AI responses.**  



---  

## Table of Contents
1. [Project Overview](#project-overview)  
2. [Key Features](#key-features)  
3. [Architecture Diagram](#architecture-diagram)  
4. [Tech Stack](#tech-stack)  
5. [Getting Started](#getting-started)  
   - Prerequisites  
   - Clone & Configure  
   - Run Locally (Docker)  
6. [API Reference](#api-reference)  
7. [Data Flow Details](#data-flow-details)  
8. [Production‑Ready Add‑Ons](#production-ready-add-ons)  
9. [Testing & CI/CD](#testing--cicd)  
10. [Roadmap & Extensions](#roadmap--extensions)  
11. [License](#license)  
12. [Acknowledgements](#acknowledgements)  

---  

## Project Overview
Modern marketing teams need more than keyword search. They want to ask *why* and *how* questions about campaign performance and receive concise, data‑driven answers. This repository implements a **Retrieval‑Augmented Generation (RAG)** pipeline:

1. **Ingest** campaign files → extract raw text.  
2. **Chunk** the text into manageable pieces (≈500 tokens).  
3. **Embed** each chunk with an LLM embedding model (OpenAI, Cohere, etc.).  
4. **Store** embeddings + metadata in PostgreSQL using the `pgvector` extension.  
5. **Query**: embed the user question, retrieve top‑k similar chunks, and feed them to an LLM to generate a context‑aware answer.  

All of this lives behind a clean FastAPI interface that can be containerised, CI‑tested, and deployed to any cloud provider.

---  

## Key Features
- **Multi‑format ingestion** – PDF, CSV, JSON, plain‑text, and any future format you add.  
- **Semantic search** using `pgvector` + IVF‑FLAT indexing for sub‑second latency.  
- **RAG pipeline** powered by LangChain (or LlamaIndex) for flexible prompt engineering.  
- **Streaming responses** – the client receives tokens as they are generated.  
- **Docker‑first** – one‑command local development and production deployment.  
- **CI/CD** with GitHub Actions (lint, unit tests, Docker image build).  
- **Optional extras** – Redis cache, JWT authentication, rate limiting, OpenTelemetry tracing.  

---  

## Architecture Diagram
```
+-------------------+        +-------------------+        +-------------------+
|   Front‑end UI    |  <---> |   FastAPI Server  |  <---> |   PostgreSQL +    |
| (React / CLI /   |        |   (Python)        |        |   pgvector       |
|  Postman)         |        |                   |        |                   |
+-------------------+        +-------------------+        +-------------------+
         |                           |                           |
         |   1. Upload file          |   2. Store embeddings     |
         |   2. Send query           |   3. Retrieve chunks      |
         |   3. Stream answer        |   4. Call LLM API         |
         v                           v                           v
+-------------------+        +-------------------+        +-------------------+
|  PDF/CSV/JSON     |  -->   |  LangChain /      |  -->   |  OpenAI / Cohere  |
|  Extractor       |        |  LlamaIndex       |        |  Embedding / Chat |
+-------------------+        +-------------------+        +-------------------+
```

---  

## Tech Stack
| Layer | Technology |
|------|------------|
| **API** | FastAPI, Uvicorn |
| **LLM / Embeddings** | OpenAI `text-embedding-ada-002`, `gpt‑4o‑mini` (or any compatible provider) |
| **Vector DB** | PostgreSQL + pgvector |
| **Chunking / Prompt** | LangChain (or LlamaIndex) |
| **Containerisation** | Docker, Docker‑Compose |
| **CI/CD** | GitHub Actions |
| **Optional Caching** | Redis |
| **Auth** | JWT (FastAPI‑Security) |
| **Logging / Metrics** | structlog, Prometheus client (optional) |
| **Testing** | pytest, httpx |
| **Docs** | Swagger UI (auto‑generated by FastAPI) |

---  

## Getting Started  

### Prerequisites
- **Docker ≥ 20.10** (includes Docker‑Compose)  
- **Python 3.11** (if you want to run without Docker)  
- **OpenAI API key** (or any other embedding/LLM provider)  
- **PostgreSQL** with the `pgvector` extension (Docker image handles this)  

### Clone & Configure
```bash
git clone https://github.com/your‑org/campaign‑rag‑backend.git
cd campaign-rag-backend
# copy the example env file and edit values
cp .env.example .env
# set OPENAI_API_KEY, DATABASE_URL, etc.
```

### Run Locally (Docker)
```bash
docker compose up --build
```
- API available at **http://localhost:8000**  
- Swagger UI at **http://localhost:8000/docs**  

### Run without Docker (for debugging)
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --reload
```

---  

## API Reference  

| Method | Endpoint | Description | Request Body | Response |
|--------|----------|-------------|--------------|----------|
| `POST` | `/ingest` | Upload a campaign document. | `multipart/form-data` (`file`) | `{ "doc_id": "<uuid>", "status": "processing" }` |
| `POST` | `/query` | Ask a natural‑language question. | `{ "question": "string" }` | **Streaming** plain‑text answer (token‑by‑token) |
| `GET` | `/health` | Simple health check. | – | `{ "status": "ok" }` |
| `GET` | `/docs` | Swagger UI (auto‑generated). | – | UI |

> **Tip:** Use the `curl` examples in `examples/` for quick testing.

---  

## Data Flow Details  

### 1️⃣ Ingestion
1. **File upload** → FastAPI receives `UploadFile`.  
2. **Format detection** → PDF → `pdfminer.six`; CSV → `pandas`; JSON → `json`.  
3. **Text extraction** → raw string.  
4. **Chunking** → `RecursiveCharacterTextSplitter` (size = 500 tokens, overlap = 50).  
5. **Embedding** → `OpenAIEmbeddings().embed_documents(chunks)`.  
6. **Store** → `PGVector.add_texts()` writes vectors + metadata (`doc_id`, `chunk_idx`) to Postgres.  

### 2️⃣ Query
1. **User question** → embed with same model.  
2. **Similarity search** → `PGVector.similarity_search_by_vector(query_vec, k=5)`.  
3. **Prompt assembly** → System prompt + retrieved chunks + question.  
4. **LLM call** → `openai.ChatCompletion.create(..., stream=True)`.  
5. **Streaming** → FastAPI `StreamingResponse` pushes tokens to client.  

---  

## Production‑Ready Add‑Ons  

| Feature | Why it matters | How to enable |
|---------|----------------|---------------|
| **JWT Auth** | Secure endpoints, multi‑tenant support. | Add `fastapi.security.HTTPBearer` middleware; configure secret in `.env`. |
| **Redis Cache** | Reduce repeated vector searches for identical queries. | Wrap `vector_store.similarity_search_by_vector` with a `@cached` decorator (TTL = 5 min). |
| **Rate Limiting** | Prevent abuse. | Use `slowapi` or `starlette-limiter`. |
| **OpenTelemetry** | Trace request latency across ingestion → LLM. | Add `opentelemetry-instrumentation-fastapi`. |
| **Prometheus Metrics** | Observe query latency, error rates. | Include `prometheus_fastapi_instrumentator`. |
| **Kubernetes Manifests** | Scale horizontally. | Provide `deployment.yaml` and `service.yaml` in `k8s/`. |
| **CI/CD** | Automated testing & image publishing. | GitHub Actions workflow already set up (`.github/workflows/ci.yml`). |

---  

## Testing & CI/CD  

- **Unit tests** (`tests/`) cover ingestion, chunking, and vector storage.  
- **Integration tests** use `httpx.AsyncClient` against a temporary Docker Compose stack.  
- **GitHub Actions** workflow runs:  
  1. Lint (`ruff`)  
  2. Type check (`mypy`)  
  3. Tests (`pytest`)  
  4. Build & push Docker image (optional)  

Run locally:  

```bash
pytest -vv
```

---  

## Roadmap & Extensions  

| Milestone | Description |
|-----------|-------------|
| **v1.0** | Core ingestion + query pipeline (completed). |
| **v1.1** | Add Redis cache, JWT auth, and rate limiting. |
| **v1.2** | Multi‑LLM support (e.g., Anthropic, Gemini). |
| **v2.0** | UI dashboard (React) for uploading files and chatting. |
| **v2.1** | Batch analytics endpoint (summarize all campaigns). |
| **v2.2** | Fine‑tuned domain‑specific LLM for higher accuracy. |
| **v3.0** | Multi‑tenant SaaS deployment with per‑tenant PostgreSQL schemas. |

---  

## License  
MIT License – feel free to adapt, extend, and commercialise. See `LICENSE` for details.

---  

## Acknowledgements  
- **LangChain** – for the clean RAG abstractions.  
- **pgvector** – for seamless vector search inside PostgreSQL.  
- **FastAPI** – for rapid API development and automatic docs.  
- **OpenAI** – for embedding and chat models used in the reference implementation.  

---  
